# ConcurrentHashMap原理分析

	本文基于JDK1.8对ConcurrentHashMap进行分析。
## 概述
+ 背景介绍
	
	1. HashMap在put的时候，插入的元素超过了容量的范围就会触发扩容操作并且rehash，这个操作会重新将原数组的内容重新hash到新的扩容数组中；
	2. 在多线程的环境下，存在同时其他的元素也在进行put操作，如果hash值相同，可能出现同时在同一数组下用链表表示，造成闭环，导致在get时会出现死循环，所以HashMap是线程不安全的。


+ 出现原因
	
	由于HashMap是线程不安全的，所以有了线程安全的版本Hashtable，然而Hashtable是在对容器进行修改操作时，synchronized整个容器，极大的降低了吞吐量。在同时满足线程安全，高吞吐量的背景下，出现了ConcurrentHashMap，在并发场景下推荐使用的并发容器。


## 数据结构

+ JDK1.7

	在JDK1.7中，是使用分段锁技术来实现的。ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成。Segment数组就是将一个大的table分割成多个小的table来进行加锁，也就是上面的提到的分段锁技术，而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样。

+ JDK1.8

	在JDK1.8中，已经摒弃了Segment的概念，而是直接用Node数组+链表/红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，从结构上看，就是优化过且线程安全的HashMap。由于HashMap的具体实现已经写过，这里不具体介绍。

## 主要源码分析

### put方法

	public V put(K key, V value) {
        return putVal(key, value, false);
    }

    final V putVal(K key, V value, boolean onlyIfAbsent) {
        if (key == null || value == null) throw new NullPointerException();
        int hash = spread(key.hashCode());//1.两次hash，减少hash冲突，使分布均匀
        int binCount = 0;
        for (Node<K,V>[] tab = table;;) {//对当前table进行自旋，直到put成功
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                tab = initTable();//2.如果table为空，初始化table
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    break;                   // 3.如果当前index上数据为空，CAS无锁插入
            }
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);//4.如果正在进行扩容，就帮助扩容
            else {
                V oldVal = null;//5.如果以上条件都不满足，那就要进行加锁操作，也就是存在hash冲突，锁住链表或者红黑树的头结点
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {//如果是链表节点，按链表节点操作
                            binCount = 1;
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    break;
                                }
                                Node<K,V> pred = e;
                                if ((e = e.next) == null) {
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    break;
                                }
                            }
                        }
                        else if (f instanceof TreeBin) {//如果是红黑树节点，按红黑树节点操作
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                if (binCount != 0) {//6.如果链表长度大于8，将链表转化成红黑树
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        addCount(1L, binCount);//7.统计当前Size，并判断是否需要扩容
        return null;
    }

下面按步骤进行分析：

+ 步骤一：spread()，两次hash，减少hash冲突，使分布均匀

		static final int spread(int h) {
	        return (h ^ (h >>> 16)) & HASH_BITS;
	    }

+ 步骤二：initTable(),如果没有初始化就先调用initTable（）方法来进行初始化过程

	    private final Node<K,V>[] initTable() {
	        Node<K,V>[] tab; int sc;
	        while ((tab = table) == null || tab.length == 0) {//对空table进行初始化操作
	            if ((sc = sizeCtl) < 0)
	                Thread.yield();//sizeCtl<0表示其他线程已经在初始化了或者扩容了，挂起当前线程
	            else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
						//CAS操作SIZECTL为-1，表示初始化状态，并且由当前线程执行初始化操作
	                try {
	                    if ((tab = table) == null || tab.length == 0) {
	                        int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
	                        @SuppressWarnings("unchecked")
	                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];//初始化
	                        table = tab = nt;
	                        sc = n - (n >>> 2);//触发扩容的阈值。如当前容量16，则阈值16*0.75 = 12
	                    }
	                } finally {
	                    sizeCtl = sc;
	                }
	                break;
	            }
	        }
	        return tab;
	    }

+ 步骤三：如果没有hash冲突就直接CAS插入

+ 步骤四：helpTransfer(),如果还在进行扩容操作就先进行扩容

		//帮助从旧的table的元素复制到新的table中,帮助转移
		final Node<K,V>[] helpTransfer(Node<K,V>[] tab, Node<K,V> f) {
	        Node<K,V>[] nextTab; int sc;
	        if (tab != null && (f instanceof ForwardingNode) &&
	            (nextTab = ((ForwardingNode<K,V>)f).nextTable) != null) {//新的table已经存在才能帮助转移
	            int rs = resizeStamp(tab.length);
	            while (nextTab == nextTable && table == tab &&
	                   (sc = sizeCtl) < 0) {
	                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
	                    sc == rs + MAX_RESIZERS || transferIndex <= 0)
	                    break;
	                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {
	                    transfer(tab, nextTab);
	                    break;
	                }
	            }
	            return nextTab;
	        }
	        return table;
	    }

	

+ 步骤五：如果存在hash冲突，synchronized加锁来保证线程安全。插入方式与HashMap相同。
	
	如果头节点是链表节点，按照链表形式直接遍历到尾端插入

	如果是红黑树节点，按照红黑树结构插入


+ 步骤六：如果上一步是按照链表方式插入，则需要判断，如果该链表的数量大于阈值8，就要先转换成黑红树的结构

+ 步骤七：如果添加成功就调用addCount（）方法统计size，并且检查是否需要扩容


### get方法

	public V get(Object key) {
        Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
        int h = spread(key.hashCode());//计算hash值
        if ((tab = table) != null && (n = tab.length) > 0 &&
            (e = tabAt(tab, (n - 1) & h)) != null) {//获取第一个元素
            if ((eh = e.hash) == h) {
                if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                    return e.val; //1.如果是第一个元素就直接返回
            }
			//2.hash值为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable来查找，查找到就返回
            else if (eh < 0) 
                return (p = e.find(h, key)) != null ? p.val : null;
            while ((e = e.next) != null) {//3.既不是首节点也不是ForwardingNode，那就往下遍历
                if (e.hash == h &&
                    ((ek = e.key) == key || (ek != null && key.equals(ek))))
                    return e.val;
            }
        }
        return null;
    }

### size方法

		public int size() {
		    long n = sumCount();
		    return ((n < 0L) ? 0 :
		            (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE :
		            (int)n);
		}
		final long sumCount() {
		    CounterCell[] as = counterCells; CounterCell a; //变化的数量
		    long sum = baseCount;
		    if (as != null) {
		        for (int i = 0; i < as.length; ++i) {
		            if ((a = as[i]) != null)
		                sum += a.value;
		        }
		    }
		    return sum;
		}

其实在并发集合中去计算size是没有多大的意义的，因为size是实时在变的，只能计算某一刻的大小，但是某一刻太快了，所以并不是很精确。


## 总结
+ JDK1.8中的ConcurrentHashMap与HashMap的数据结构类似，都是通过数组+链表/红黑树来实现的；

+ ConcurrentHashMap基于乐观锁来实现，只有当发生hash冲突的时候，才使用synchronized来进行头节点的锁定；

+ ConcurrentHashMap对比Hashtable拥有乐观锁的使用，以及更细粒度的锁控制，因此，在选择并发Map时，应使用ConcurrentHashMap。